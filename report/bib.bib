% Encoding: UTF-8
% I fetched this from the NASA ADS
% They gave it the catchy name on the following line; I changed it to Einstein1905, but I could have called it anything: E05, SpecRel, bob...
%1905AnP...323..639E,
@ARTICLE{Einstein1905,
	author	= {{Einstein}, Albert},
	title		= "{Ist die Tr{\"a}gheit eines K{\"o}rpers von seinem Energieinhalt abh{\"a}ngig?}",
	journal	= {Annalen der Physik},
	year		= 1905,
	volume	= 323,
	pages	= {639-641},
	doi		= {10.1002/andp.19053231314},
	adsurl	= {http://ukads.nottingham.ac.uk/cgi-bin/nph-bib_query?bibcode=1905AnP...323..639E&db_key=AST},
	adsnote	= {Provided by the Smithsonian/NASA Astrophysics Data System}
}

@InProceedings{Pennington2014,
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  title     = {Glove: Global vectors for word representation},
  booktitle = {Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  year      = {2014},
  pages     = {1532--1543},
}

@Article{Tian2018,
  author  = {Tian, Kevin and Zhang, Teng and Zou, James},
  title   = {CoVeR: Learning Covariate-Specific Vector Representations with Tensor Decompositions},
  journal = {arXiv preprint arXiv:1802.07839},
  year    = {2018},
}

@Article{Mikolov2013,
  author  = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title   = {Efficient estimation of word representations in vector space},
  journal = {arXiv preprint arXiv:1301.3781},
  year    = {2013},
}

@Article{Harris1954,
  author    = {Harris, Zellig S},
  title     = {Distributional structure},
  journal   = {Word},
  year      = {1954},
  volume    = {10},
  number    = {2-3},
  pages     = {146--162},
  publisher = {Taylor \& Francis},
}

@Article{Bengio2003,
  author     = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
  title      = {A Neural Probabilistic Language Model},
  journal    = {J. Mach. Learn. Res.},
  year       = {2003},
  volume     = {3},
  pages      = {1137--1155},
  month      = mar,
  issn       = {1532-4435},
  acmid      = {944966},
  issue_date = {3/1/2003},
  numpages   = {19},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=944919.944966},
}

@InProceedings{Socher2013,
  author    = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  title     = {Recursive deep models for semantic compositionality over a sentiment treebank},
  booktitle = {Proceedings of the 2013 conference on empirical methods in natural language processing},
  year      = {2013},
  pages     = {1631--1642},
}

@InProceedings{Socher2013a,
  author    = {Socher, Richard and Bauer, John and Manning, Christopher D and others},
  title     = {Parsing with compositional vector grammars},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2013},
  volume    = {1},
  pages     = {455--465},
}

@InProceedings{Mikolov2010,
  author    = {Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  title     = {Recurrent neural network based language model},
  booktitle = {Eleventh annual conference of the international speech communication association},
  year      = {2010},
}

@Article{Hinton1986,
  author  = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  title   = {Learning representations by back-propagating errors},
  journal = {Cognitive modeling},
  year    = {1986},
}

@Article{Hochreiter1997,
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  title     = {Long short-term memory},
  journal   = {Neural computation},
  year      = {1997},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  publisher = {MIT Press},
}

@InProceedings{Mikolov2013a,
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  title     = {Distributed representations of words and phrases and their compositionality},
  booktitle = {Advances in neural information processing systems},
  year      = {2013},
  pages     = {3111--3119},
}

@InProceedings{Joachims1998,
  author       = {Joachims, Thorsten},
  title        = {Text categorization with support vector machines: Learning with many relevant features},
  booktitle    = {European conference on machine learning},
  year         = {1998},
  pages        = {137--142},
  organization = {Springer},
}

@Misc{Hastie2009,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  title     = {The elements of statistical learning: data mining, inference, and prediction, Springer Series in Statistics},
  year      = {2009},
  publisher = {Springer New York},
}

@Misc{Cortes1995,
  author   = {Corinna Cortes, Vladimir Vapnik},
  title    = {Support-Vector Networks},
  year     = {1995},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the supportvector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
}

@Misc{Lewis2004,
  author        = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
  title         = {RCV1: A new benchmark collection for text categorization research},
  year          = {2004},
  __markedentry = {[akin:]},
  abstract      = {Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collectionâ€™s properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as},
}

@Article{Dai2015a,
	author        = {Andrew M. Dai and Quoc V. Le},
	title         = {Semi-supervised Sequence Learning},
	__markedentry = {[akin:6]},
	abstract      = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
	year          = {2015},
}

@article{Srivastava2014,
	author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	journal = {Journal of Machine Learning Research},
	year    = {2014},
	volume  = {15},
	pages   = {1929-1958},
	url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@book{McCrum2011,
	title={The Story of English},
	author={McCrum, R.},
	isbn={9780571275083},
	series={BBC books},
	url={https://books.google.co.uk/books?id=PWtWewAACAAJ},
	year={2011},
	publisher={Faber \& Faber}
}

@inproceedings{Potash2015,
	author = {Potash, Peter and Romanov, Alexey and Rumshisky, Anna},
	year = {2015},
	month = {01},
	pages = {1919-1924},
	title = {GhostWriter: Using an LSTM for Automatic Rap Lyric Generation},
	doi = {10.18653/v1/D15-1221}
}

@inproceedings{Zhang2014,
	title={Chinese poetry generation with recurrent neural networks},
	author={Zhang, Xingxing and Lapata, Mirella},
	booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={670--680},
	year={2014}
}

@book{Edwards2009,
	title={How to Rap},
	author={Edwards, P. and Rap, K.G.},
	isbn={9781569763773},
	url={https://books.google.co.uk/books?id=ZemzPXFgeEoC},
	year={2009},
	publisher={Chicago Review Press, Incorporated}
}

@inproceedings{Irvin2016,
	title={Recurrent Neural Networks with Attention for Genre Classification},
	author={Jeremy Irvin and Elliott Chartock and Nicolas Hollander},
	year={2016}
}

@proceeding{Pui2018,
	author = { Chun Pui  Tang,Ka Long  Chui,Ying Kin  Yu,Zhiliang  Zeng,Kin Hong  Wong},
	title = {Music genre classification using a hierarchical long short term memory (LSTM) model},
	volume = {10828},
	year = {2018},
	doi = {10.1117/12.2501763},
	URL = {https://doi.org/10.1117/12.2501763},
}

@book{Manning2008,
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
	title = {Introduction to Information Retrieval},
	year = {2008},
	isbn = {0521865719, 9780521865715},
	publisher = {Cambridge University Press},
	address = {New York, NY, USA},
}

@article{Qi2018,
	author = {Qi, Ye and Singh Sachan, Devendra and Felix, Matthieu and Janani Padmanabhan, Sarguna and Neubig, Graham},
	year = {2018},
	title = {When and Why are Pre-trainedWord Embeddings Useful for Neural Machine Translation?}
}

@inproceedings{Tsaptsinos2017,
	title={Music Genre Classification by Lyrics using a Hierarchical Attention Network},
	author={Alexandros Tsaptsinos},
	year={2017}
}

@inproceedings{Yang2016,
	title={Hierarchical attention networks for document classification},
	author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
	booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	pages={1480--1489},
	year={2016}
}

@article{Bojanowski2017,
	title={Enriching word vectors with subword information},
	author={Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	journal={Transactions of the Association for Computational Linguistics},
	volume={5},
	pages={135--146},
	year={2017},
	publisher={MIT Press}
}

@article{Duchi2011,
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	journal = {J. Mach. Learn. Res.},
	issue_date = {2/1/2011},
	volume = {12},
	month = jul,
	year = {2011},
	issn = {1532-4435},
	pages = {2121--2159},
	numpages = {39},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
	acmid = {2021068},
	publisher = {JMLR.org},
}

@inproceedings{Rush2015,
	title={A Neural Attention Model for Abstractive Sentence Summarization},
	author={Alexander M. Rush and Sumit Chopra and Jason Weston},
	booktitle={EMNLP},
	year={2015},
}

@article{Katz1987,
	author={S. Katz},
	journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
	title={Estimation of probabilities from sparse data for the language model component of a speech recognizer},
	year={1987},
	volume={35},
	number={3},
	pages={400-401},
	keywords={Natural languages;Speech recognition;Recursive estimation;Maximum likelihood estimation;Probability;Statistics;Speech processing;Acoustic signal processing},
	doi={10.1109/TASSP.1987.1165125},
	ISSN={0096-3518},
	month={March},
}

@incollection{Rumelhart1988,
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	chapter = {Learning Representations by Back-propagating Errors},
	title = {Neurocomputing: Foundations of Research},
	editor = {Anderson, James A. and Rosenfeld, Edward},
	year = {1988},
	isbn = {0-262-01097-6},
	pages = {696--699},
	numpages = {4},
	url = {http://dl.acm.org/citation.cfm?id=65669.104451},
	acmid = {104451},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}

@ARTICLE{Bengio1994,
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},
	journal={IEEE Transactions on Neural Networks},
	title={Learning long-term dependencies with gradient descent is difficult},
	year={1994},
	volume={5},
	number={2},
	pages={157-166},
	keywords={recurrent neural nets;learning (artificial intelligence);numerical analysis;long-term dependencies;gradient descent;recognition;production problems;prediction problems;recurrent neural network training;temporal contingencies;input/output sequence mapping;efficient learning;Recurrent neural networks;Production;Delay effects;Intelligent networks;Neural networks;Discrete transforms;Computer networks;Cost function;Neurofeedback;Displays},
	doi={10.1109/72.279181},
	ISSN={1045-9227},
	month={March},
}

@inproceedings{Sahami1998,
	title={A Bayesian approach to filtering junk e-mail},
	author={Sahami, Mehran and Dumais, Susan and Heckerman, David and Horvitz, Eric},
	booktitle={Learning for Text Categorization: Papers from the 1998 workshop},
	volume={62},
	pages={98--105},
	year={1998},
	organization={Madison, Wisconsin}
}

@article{Kingma2014,
	title={Adam: A Method for Stochastic Optimization},
	author={Diederik P. Kingma and Jimmy Ba},
	journal={CoRR},
	year={2014},
	volume={abs/1412.6980}
}







@Comment{jabref-meta: databaseType:bibtex;}

{<key>,
% Required
	author	= {},
	title		= {},
	school	= {},
	year		= {},
% Optional
	type		= {},
	address	= {},
	month	= {},
	note		= {}
}
