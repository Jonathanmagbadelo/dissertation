%-----------------------------------------------------
% Chapter: Evaluation
%-----------------------------------------------------
\chapter{Evaluation}
\label{chap:evaluation}
\section{Validating CoVeR Implementation}
As previously stated, there currently does not exist any public implementation of the CoVeR algorithm. As a basis for validating the quality of the CoVeR model created in this project, the base embeddings generated were compared against embeddings generated using the Glove algorithm. In order to achieve this, a simple framework was set up to compare the the most similar words for each word in both the CoVeR and GloVe embeddings. For this experiment, each model was generated by using a symmetric context window of size 8 and each had a dimensionality of 50. Staying true to both papers, the Adam optimiser was used to train the CoVeR algorithm, whilst the AdaGrad optimiser was used to train GloVe. Both models were trained for 10 epochs. 

\noindent
\newline
To find the most similar words for a given word in both the CoVeR and GloVe model, the cosine similarity measure was used. 

\begin{equation}
sim(A, B) =\dfrac{A \cdot B}{\lVert A \rVert \lVert B \rVert}
\end{equation}

\noindent
For each word, the top \(n\) most similar words were calculated in both the GloVe and CoVer model and for each of these precision, recall and F1 scores were calculated. The results of these can be found in Table 8.1.

\begin{table}[ht]
	\centering
	\begin{tabular}{ | l | l | l | l | }
		\hline
		\textbf{\(n\)} & \textbf{Precision} & \textbf{Recall}  & \textbf{F1}\\ \hline
		1 & 0.51 & 0.51 & 0.51\\ \hline
		5 & 0.33 & 0.32 & 0.3\\ \hline
		10 & 0.29 & 0.28 & 0.26\\ \hline
		20 & 0.25 & 0.25 & 0.24\\ \hline
		50 & 0.23 & 0.22 & 0.22\\ \hline
	\end{tabular}
	\label{Tab:CoverPRF}
	\caption[Precision, Recall and F1 Scores for Base CoVeR Embeddings]{Precision, Recall and F1 scores for base CoVeR embeddings.}
\end{table}


\section{Model Evaluations}
\subsection{Language Model}
Typically, language models can be evaluated using either extrinsic or intrinsic methods. Whilst extrinsic evaluation methods measure the performance of a model when applied to an application, intrinsic methods allow for the evaluation of a model independent of any application. A drawback of using extrinsic evaluation is that it is computationally expensive task, as it involves the the full deployment of an application which then has to be evaluated using task specific measures.
\subsubsection{Intrinsic Evaluation}
A typical intrinsic evaluation metric used to evaluate a language model is perplexity(REF 19, 30, 43). Perplexity is a measure of how well a given language model will predict test data. The general formula for perplexity is given below

\noindent
\newline
To calculate the perplexity of each model, a 20\% validation set was set aside during the training of each model and the perplexity was calcaluted at the end of every epoch. Full graphs for each of these can be found in \autoref{app:code} 

\noindent
\newline


\subsection{Text Classification}
In order to further evaluate the genre specific word embeddings produced by CoVeR, each genre embedding was used to train a binary classifier that would discriminate lyrics as either belonging to that particular genre or not. These were compared with GloVe trained word embeddings to see if the the genre specific word embeddings did indeed capture the semantics of the genre better than generic glove embeddings. The results of the test can be seen below

\begin{table}[ht]
	\centering
	\begin{tabular}{ | p{3cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} |}
		\hline
		\textbf{Embedding} & \textbf{Training Loss} & \textbf{Training Accuracy} & \textbf{Validation Loss} & \textbf{Validation Accuracy}\\ \hline
		GloVe & 0.5807 & 0.6906 & 0.6194 & 0.6698\\ \hline
		Pop Covariate & 0.5791 & 0.6925 & 0.6122 & 0.6730\\ \hline
	\end{tabular}
	\label{Tab:GlovePopClass}
	\caption{GlovePopClass}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{ | p{3cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} |}
		\hline
		\textbf{Embedding} & \textbf{Training Loss} & \textbf{Training Accuracy} & \textbf{Validation Loss} & \textbf{Validation Accuracy}\\ \hline
		GloVe & 0.5251 & 0.7238 & 0.5691 & 0.6985\\ \hline
		Rock Covariate & 0.5200 & 0.7260 & 0.5646 & 0.7009\\ \hline
	\end{tabular}
	\label{Tab:GloveRockClass}
	\caption{GloveRockClass}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{ | p{3cm} | p{2cm} | p{2cm} | p{2cm} | p{2cm} |}
		\hline
		\textbf{Embedding} & \textbf{Training Loss} & \textbf{Training Accuracy} & \textbf{Validation Loss} & \textbf{Validation Accuracy}\\ \hline
		GloVe & 0.4511 & 0.7926 & 0.5115 & 0.7760\\ \hline
		Hip Hop Covariate & 0.4593 & 0.7878 & 0.4985 & 0.7823\\ \hline
	\end{tabular}
	\label{Tab:GloveHipHopClass}
	\caption{GloveHipHopClass}
\end{table}
\section{SONGIFAI}
\subsection{Requirements Evaluation}
\subsubsection{Functional}
\begin{table}[h]
	\centering
	\begin{tabular}{ | l | p{10cm} | l | }
		\hline
		\textbf{ID} & \textbf{Description} & \textbf{Achieved} \\ \hline
		FR1 & The system should allow users to input lyrics & Yes \\ \hline
		FR2 & The system should allow users to edit, save and delete lyrics & Yes  \\ \hline
		FR3 & The system should be able to classify user submitted lyrics as either Pop/Rock/Hip Hop & Yes \\ \hline
		FR4 & The system should be be able to suggest words from a given word. These words should be the most similar words in the chosen covariate word embedding space & Yes \\ \hline
		FR5 & The system should be able to provide real time text prediction whilst a user is in edit mode & Yes \\ \hline
		FR6 & The system should allow for the filtering of explicit content in both the word suggestion and word prediction feature & Yes \\ \hline
		FR7 & The user should be able to change the underlying covariate specific word embeddings used or the base embeddings if desired & Yes \\ \hline
	\end{tabular}
	\label{Tab:Tcru}
	\caption[Functional Requirements]{SONGIFAI Functional Requirements}
\end{table}
\subsubsection{Non-Functional}
\begin{table}[ht]
	\centering
	\begin{tabular}{ | l | p{10cm} | l | }
		\hline
		\textbf{ID} & \textbf{Description} & \textbf{Achieved} \\ \hline
		NFR1 & The system should take the form of a web application, compatible on multiple device types & Partial \\ \hline
		NFR2 & The word prediction feature should return a list of candidate words with minimal latency & Yes \\ \hline
		NFR3 & The word suggestion feature should return a list of suggested words with minimal latency & Yes \\ \hline
	\end{tabular}
	\label{Tab:Tcr}
	\caption[Non-Functional Requirements]{SONGIFAI Non-Functional Requirements}
\end{table}
\subsubsection{Extensions}
\begin{table}[ht]
	\centering
	\begin{tabular}{ | l | p{10cm} | l | }
		\hline
		\textbf{ID} & \textbf{Description} & \textbf{Achieved} \\ \hline
		E1 & The system may allow rhyme words to be suggested in the word suggestion feature if time permits& No \\ \hline
		E2 & The system may allow sub genres to be selected for embeddings, if time and data permits& No \\ \hline
	\end{tabular}
	\label{Tab:Tcr}
	\caption[Possible Extensions]{SONGIFAI Possible Extensions}
\end{table}
