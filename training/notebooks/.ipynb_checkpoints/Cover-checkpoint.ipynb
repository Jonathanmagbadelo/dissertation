{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "class Cover:\n",
    "    def __init__(self, window_size=5, min_occurrence_count=1):\n",
    "        self.token_to_id = {}\n",
    "        self.window_size = window_size\n",
    "        self.min_occurrence_count = min_occurrence_count\n",
    "        self.transformed_data = []\n",
    "        self.corpus = None\n",
    "    \n",
    "    def import_data(self, filename, column_name):\n",
    "        data_frame = pd.read_csv(filename, encoding='latin-1')\n",
    "        self.corpus = data_frame[column_name].astype(str).tolist()\n",
    "        print(\"Corpus has {} documents\", len(self.corpus))\n",
    "        print(self.corpus[0])\n",
    "        \n",
    "    def _get_or_set_token_to_id(self, word):\n",
    "        try:\n",
    "            return self.token_to_id[word]\n",
    "        except KeyError:\n",
    "            idx = len(self.token_to_id)\n",
    "            self.token_to_id[word] = idx\n",
    "            return idx\n",
    "        \n",
    "    def fit_transform(self):\n",
    "        if self.corpus is None:\n",
    "            print(\"Please load corpus first!!\")\n",
    "        else:\n",
    "            # tokenised_documents = [document.lower().strip(punctuation).split(' ') for document in self.corpus]\n",
    "            tokenised_documents = (document.split(' ') for document in self.corpus)\n",
    "            print(\"Done tokenising\")\n",
    "            \n",
    "            word_occurrences = {\n",
    "                token : count \n",
    "                for token, count in Counter(chain.from_iterable(tokenised_documents)).items()\n",
    "                if count >= self.min_occurrence_count\n",
    "            }\n",
    "            \n",
    "            print(\"print created word occurs\")\n",
    "            \n",
    "            self.transformed_data = [[self._get_or_set_token_to_id(word) if word in word_occurrences else 0 for word in sentence] for sentence in tokenised_documents]\n",
    "            \n",
    "            print(\"Corpus has {} documents\", len(self.transformed_data))\n",
    "    \n",
    "    def build_cooccur_matrix(self):\n",
    "        ij_list = []\n",
    "        cooccur_matrix = np.fromiter(())      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has {} documents 362237\n",
      "Oh baby, how you doing?\n",
      "You know I'm gonna cut right to the chase\n",
      "Some women were made but me, myself\n",
      "I like to think that I was created for a special purpose\n",
      "You know, what's more special than you? You feel me\n",
      "It's on baby, let's get lost\n",
      "You don't need to call into work 'cause you're the boss\n",
      "For real, want you to show me how you feel\n",
      "I consider myself lucky, that's a big deal\n",
      "Why? Well, you got the key to my heart\n",
      "But you ain't gonna need it, I'd rather you open up my body\n",
      "And show me secrets, you didn't know was inside\n",
      "No need for me to lie\n",
      "It's too big, it's too wide\n",
      "It's too strong, it won't fit\n",
      "It's too much, it's too tough\n",
      "He talk like this 'cause he can back it up\n",
      "He got a big ego, such a huge ego\n",
      "I love his big ego, it's too much\n",
      "He walk like this 'cause he can back it up\n",
      "Usually I'm humble, right now I don't choose\n",
      "You can leave with me or you could have the blues\n",
      "Some call it arrogant, I call it confident\n",
      "You decide when you find on what I'm working with\n",
      "Damn I know I'm killing you with them legs\n",
      "Better yet them thighs\n",
      "Matter a fact it's my smile or maybe my eyes\n",
      "Boy you a site to see, kind of something like me\n",
      "It's too big, it's too wide\n",
      "It's too strong, it won't fit\n",
      "It's too much, it's too tough\n",
      "I talk like this 'cause I can back it up\n",
      "I got a big ego, such a huge ego\n",
      "But he love my big ego, it's too much\n",
      "I walk like this 'cause I can back it up\n",
      "I, I walk like this 'cause I can back it up\n",
      "I, I talk like this 'cause I can back it up\n",
      "I, I can back it up, I can back it up\n",
      "I walk like this 'cause I can back it up\n",
      "It's too big, it's too wide\n",
      "It's too strong, it won't fit\n",
      "It's too much, it's too tough\n",
      "He talk like this 'cause he can back it up\n",
      "He got a big ego, such a huge ego, such a huge ego\n",
      "I love his big ego, it's too much\n",
      "He walk like this 'cause he can back it up\n",
      "Ego so big, you must admit\n",
      "I got every reason to feel like I'm that bitch\n",
      "Ego so strong, if you ain't know\n",
      "I don't need no beat, I can sing it with piano\n",
      "Done tokenising\n",
      "print created word occurs\n",
      "Corpus has {} documents 0\n",
      "Time taken is 18.039848804473877\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "filename = '/opt/training/data/raw/lyrics.csv'\n",
    "column_name = 'lyrics'\n",
    "cover = Cover()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "cover.import_data(filename, column_name)\n",
    "cover.fit_transform()\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "cover = Cover()\n",
    "texts = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "sentences = [\" \".join(list_of_words) for list_of_words in texts]\n",
    "\n",
    "start_time = time.time()\n",
    "data = cover.fit_transform(sentences)\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))\n",
    "print(data[1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
