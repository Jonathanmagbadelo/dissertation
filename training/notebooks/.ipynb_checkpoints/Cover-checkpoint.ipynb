{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cover Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYTHONPATH'] = '$PYTHONPATH:/opt/training'\n",
    "os.chdir('/opt/training')\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cover\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_session.sparkContext.addPyFile(\"/opt/training/src/modelling/Cover.py\")\n",
    "spark_session.sparkContext.addPyFile(\"/opt/training/src/processing/utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling import Cover\n",
    "from src.processing import utils\n",
    "filename = '/opt/training/data/raw/test-lyrics.csv'\n",
    "cover = Cover.Cover(spark_session=spark_session, embedding_size=300, x_max=100, alpha=.75, weight_decay=1, learning_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "start_time = time.time()\n",
    "cover.import_data(filename)\n",
    "cover.fit_transform(column_name='Lyrics', covariate='Genre', min_occurrence_count=5, window_size=5)\n",
    "cover.build_co_occurrence_matrix()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, ArrayType\n",
    "from pyspark.sql.functions import col, explode, sum as sum_, udf\n",
    "from itertools import chain\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"age\", MapType(ArrayType(IntegerType()), IntegerType()), False),\n",
    "    StructField(\"type\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark_session.createDataFrame([({(1, 2): 1, (2 , 2): 2}, \"one\"), ({(1, 2): 1}, \"two\"), ({(2, 2): 3} , \"one\"), ({(2, 2): 3} , \"two\")], schema)\n",
    "\n",
    "df.show(10, False)\n",
    "\n",
    "lis = df.select(\"type\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "dicto = {key : value for value, key in enumerate(lis)}\n",
    "\n",
    "type_2_num = udf(lambda x: dicto[x])\n",
    "\n",
    "print(dicto)\n",
    "\n",
    "df = df.withColumn('type', type_2_num('type').cast(IntegerType()))\n",
    "\n",
    "\n",
    "t = df.select(explode(col(\"age\")), \"type\").groupBy(col(\"key\"), col(\"type\")).agg(sum_(\"value\").alias(\"value\"))\n",
    "\n",
    "t.show()\n",
    "\n",
    "print(t.schema)\n",
    "\n",
    "x = t.select(\"value\", \"key\", \"type\").rdd.map(lambda x: (list(chain([x['type']], x.key)), x.value)).collect()\n",
    "\n",
    "x = list(zip(*x))\n",
    "\n",
    "print(x)\n",
    "\n",
    "index, value = x\n",
    "\n",
    "print(list(index)[0])\n",
    "\n",
    "print(list(value)[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 40765,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"stdin_port\": 45967,\n",
      "  \"key\": \"ca2b500b-6ea5542a7633689b24228f4d\",\n",
      "  \"control_port\": 36359,\n",
      "  \"transport\": \"tcp\",\n",
      "  \"hb_port\": 50955,\n",
      "  \"iopub_port\": 34819,\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-a08d0a49-4218-4256-90fe-0676755c928a.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
