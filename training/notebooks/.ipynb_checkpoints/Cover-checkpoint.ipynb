{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "print(os.environ.get('PYSPARK_DRIVER_PYTHON'))\n",
    "print(os.environ.get('PYSPARK_PYTHON'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cover\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col, explode, monotonically_increasing_id \n",
    "\n",
    "class Cover:\n",
    "    def __init__(self, window_size=5, min_occurrence_count=1):\n",
    "        self.token_to_id = {}\n",
    "        self.window_size = window_size\n",
    "        self.min_occurrence_count = min_occurrence_count\n",
    "        self.transformed_data = []\n",
    "        self.corpus = None\n",
    "    \n",
    "    def import_data(self, filename, column_name):\n",
    "        data_frame = pd.read_csv(filename, encoding='latin-1')\n",
    "        self.corpus = data_frame[column_name].astype(str).tolist()\n",
    "        print(\"Corpus has {} documents\", len(self.corpus))\n",
    "        print(self.corpus[0])\n",
    "        \n",
    "    def _get_or_set_token_to_id(self, word):\n",
    "        try:\n",
    "            return self.token_to_id[word]\n",
    "        except KeyError:\n",
    "            idx = len(self.token_to_id)\n",
    "            self.token_to_id[word] = idx\n",
    "            return idx\n",
    "        \n",
    "    def fit_transform(self):\n",
    "        if self.corpus is None:\n",
    "            print(\"Please load corpus first!!\")\n",
    "        else:\n",
    "            tokenised_documents = [document.lower().strip(punctuation).split(' ') for document in self.corpus]\n",
    "            #tokenised_documents = (document.split(' ') for document in self.corpus)\n",
    "            print(\"Done tokenising\")\n",
    "            \n",
    "            word_occurrences = {\n",
    "                token : count \n",
    "                for token, count in Counter(chain.from_iterable(tokenised_documents)).items()\n",
    "                if count >= self.min_occurrence_count\n",
    "            }\n",
    "            \n",
    "            print(\"print created word occurs\")\n",
    "            \n",
    "            self.transformed_data = [[self._get_or_set_token_to_id(word) if word in word_occurrences else 0 for word in sentence] for sentence in tokenised_documents]\n",
    "            \n",
    "            print(\"Corpus has {} documents\", len(self.transformed_data))\n",
    "    \n",
    "    def fit_transform_duo(self, filename):\n",
    "        dataframe = spark.read.format(\"csv\").option(\"header\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(filename)\n",
    "        \n",
    "        value_type = StructType([StructField('letter', StringType())])\n",
    "        \n",
    "        tokenise = udf(lambda x: x.lower().strip(punctuation).split(' ') if x else [], ArrayType(StringType()))\n",
    "        \n",
    "        df = dataframe.withColumn(\"tokens\", tokenise(\"Lyrics\").alias(\"tokens\"))\n",
    "        \n",
    "        words = df.withColumn('word', explode(col('tokens')))\\\n",
    "                             .groupBy('word')\\\n",
    "                             .count()\\\n",
    "                             .sort('count', ascending=True)\n",
    "        \n",
    "        words_with_id = words.withColumn('id', monotonically_increasing_id())\n",
    "        \n",
    "        words_with_id.show()\n",
    "        \n",
    "    def build_cooccur_matrix(self):\n",
    "        ij_list = []\n",
    "        cooccur_matrix = np.fromiter(())      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+\n",
      "|         word|count| id|\n",
      "+-------------+-----+---+\n",
      "|           l7|    1|  0|\n",
      "|     shooooes|    1|  1|\n",
      "|     mindtake|    1|  2|\n",
      "|     jugglers|    1|  3|\n",
      "|       versos|    1|  4|\n",
      "|       biting|    1|  5|\n",
      "|       lathen|    1|  6|\n",
      "|      methose|    1|  7|\n",
      "|    underyeah|    1|  8|\n",
      "|    blackroof|    1|  9|\n",
      "|      coldhow|    1| 10|\n",
      "|      shedded|    1| 11|\n",
      "|        ainÌ¢t|    1| 12|\n",
      "|       slaver|    1| 13|\n",
      "|differentlyby|    1| 14|\n",
      "|       spared|    1| 15|\n",
      "|     choiceme|    1| 16|\n",
      "|    whycloser|    1| 17|\n",
      "|     newlywed|    1| 18|\n",
      "|    candletop|    1| 19|\n",
      "+-------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken is 3.2167532444000244\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "filename = '/opt/training/data/raw/billboard_lyrics_1964-2015.csv'\n",
    "column_name = 'lyrics'\n",
    "cover = Cover()\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "#cover.import_data(filename, column_name)\n",
    "cover.fit_transform_duo(filename)\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "cover = Cover()\n",
    "texts = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "sentences = [\" \".join(list_of_words) for list_of_words in texts]\n",
    "\n",
    "start_time = time.time()\n",
    "data = cover.fit_transform(sentences)\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))\n",
    "print(data[1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
