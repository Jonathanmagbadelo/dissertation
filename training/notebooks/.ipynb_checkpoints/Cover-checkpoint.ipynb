{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "print(os.environ.get('PYSPARK_DRIVER_PYTHON'))\n",
    "print(os.environ.get('PYSPARK_PYTHON'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cover\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col, explode, monotonically_increasing_id, when\n",
    "\n",
    "class Cover:\n",
    "    def __init__(self, window_size=5, min_occurrence_count=1):\n",
    "        self.token_to_id = {}\n",
    "        self.window_size = window_size\n",
    "        self.min_occurrence_count = min_occurrence_count\n",
    "        self.transformed_data = []\n",
    "        self.corpus = None\n",
    "    \n",
    "    def import_data(self, filename):\n",
    "        self.corpus = spark.read.format(\"csv\").option(\"header\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(filename)\n",
    "        print(\"Corpus has {} documents\".format(self.corpus.count()))\n",
    "        \n",
    "    def fit_transform(self):\n",
    "        if self.corpus is None:\n",
    "            print()\n",
    "        else:\n",
    "            tokenise = udf(lambda x: x.lower().strip(punctuation).split(' ') if x else [], ArrayType(StringType()))\n",
    "\n",
    "            tokenised_dataframe = self.corpus.withColumn(\"tokens\", tokenise(\"Lyrics\").alias(\"tokens\"))\n",
    "\n",
    "            words_dataframe = tokenised_dataframe.withColumn('word', explode(col('tokens')))\\\n",
    "                                 .groupBy('word')\\\n",
    "                                 .count()\\\n",
    "                                 .sort('count', ascending=True)\n",
    "\n",
    "            words_with_id_dataframe = words_dataframe.withColumn('id', monotonically_increasing_id() + 1)\n",
    "\n",
    "            filtered_words_with_id_dataframe = words_with_id_dataframe.withColumn('id', when(words_with_id_dataframe['count'] <= self.min_occurrence_count, 0).otherwise(words_with_id_dataframe.id))\n",
    "\n",
    "            filtered_words_with_id_dataframe.show()\n",
    "        \n",
    "    def build_cooccur_matrix(self):\n",
    "        ij_list = []\n",
    "        cooccur_matrix = np.fromiter(())      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 5100 documents\n",
      "+-------------+-----+---+\n",
      "|         word|count| id|\n",
      "+-------------+-----+---+\n",
      "|           l7|    1|  0|\n",
      "|     shooooes|    1|  0|\n",
      "|     mindtake|    1|  0|\n",
      "|     jugglers|    1|  0|\n",
      "|       versos|    1|  0|\n",
      "|       biting|    1|  0|\n",
      "|       lathen|    1|  0|\n",
      "|      methose|    1|  0|\n",
      "|    underyeah|    1|  0|\n",
      "|    blackroof|    1|  0|\n",
      "|      coldhow|    1|  0|\n",
      "|      shedded|    1|  0|\n",
      "|        ainÌ¢t|    1|  0|\n",
      "|       slaver|    1|  0|\n",
      "|differentlyby|    1|  0|\n",
      "|       spared|    1|  0|\n",
      "|     choiceme|    1|  0|\n",
      "|    whycloser|    1|  0|\n",
      "|     newlywed|    1|  0|\n",
      "|    candletop|    1|  0|\n",
      "+-------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken is 3.2542426586151123\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "filename = '/opt/training/data/raw/billboard_lyrics_1964-2015.csv'\n",
    "column_name = 'lyrics'\n",
    "cover = Cover(min_occurrence_count=5)\n",
    "\n",
    "start_time = time.time()\n",
    "cover.import_data(filename)\n",
    "cover.fit_transform()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "cover = Cover()\n",
    "texts = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "sentences = [\" \".join(list_of_words) for list_of_words in texts]\n",
    "\n",
    "start_time = time.time()\n",
    "data = cover.fit_transform(sentences)\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))\n",
    "print(data[1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
