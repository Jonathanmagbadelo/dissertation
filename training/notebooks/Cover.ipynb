{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cover Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYTHONPATH'] = '$PYTHONPATH:/opt/training'\n",
    "os.chdir('/opt/training')\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cover\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_session.sparkContext.addPyFile(\"/opt/training/src/modelling/Cover.py\")\n",
    "spark_session.sparkContext.addPyFile(\"/opt/training/src/processing/utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modelling import Cover\n",
    "from src.processing import utils\n",
    "filename = '/opt/training/data/raw/test-lyrics.csv'\n",
    "column_names = ['Lyrics', 'Genre']\n",
    "cover = Cover.Cover(spark_session=spark_session, embedding_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 5 documents\n",
      "+-----+--------------------+\n",
      "|Genre|              Lyrics|\n",
      "+-----+--------------------+\n",
      "|  Pop|Yeah, breakfast a...|\n",
      "|  Rap|I'm all in my bag...|\n",
      "|  Rap|Cole World, Cole ...|\n",
      "|  Pop|I'm not just tryi...|\n",
      "|  Rap|Two bad bitches a...|\n",
      "+-----+--------------------+\n",
      "\n",
      "+-----+-----+---+\n",
      "| word|count| id|\n",
      "+-----+-----+---+\n",
      "|    i|   53|321|\n",
      "|  you|   34|320|\n",
      "|  the|   29|319|\n",
      "| that|   22|318|\n",
      "|  and|   20|317|\n",
      "|   to|   19|316|\n",
      "|   im|   18|315|\n",
      "|    a|   16|314|\n",
      "|   my|   16|313|\n",
      "|   it|   15|312|\n",
      "|   me|   14|311|\n",
      "|   be|   12|310|\n",
      "|  got|   12|309|\n",
      "|sorry|   11|308|\n",
      "|  for|   11|307|\n",
      "| they|   10|306|\n",
      "| what|    9|305|\n",
      "|  ass|    9|304|\n",
      "|  say|    9|303|\n",
      "| yeah|    9|302|\n",
      "+-----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "There are 322 unique tokens\n",
      "Mapped tokens to unique id\n",
      "+--------------------+-----+\n",
      "|              matrix|Genre|\n",
      "+--------------------+-----+\n",
      "|[[321, 321] -> 0....|  Pop|\n",
      "|[[314, 319] -> 0....|  Rap|\n",
      "|[[311, 317] -> 1....|  Rap|\n",
      "|[[303, 321] -> 0....|  Pop|\n",
      "|[[321, 315] -> 0....|  Rap|\n",
      "+--------------------+-----+\n",
      "\n",
      "There are 543 ij pairs\n",
      "+----------+-----+-------------------+\n",
      "|       key|Genre|              value|\n",
      "+----------+-----+-------------------+\n",
      "|[312, 307]|  Rap| 0.6666666865348816|\n",
      "|[318, 317]|  Rap| 0.8333333134651184|\n",
      "|[300, 305]|  Rap| 0.3333333432674408|\n",
      "|[300, 309]|  Rap|               0.75|\n",
      "|[308, 321]|  Pop| 2.5999999046325684|\n",
      "|[297, 317]|  Pop| 0.3333333432674408|\n",
      "|[296, 304]|  Rap| 0.3333333432674408|\n",
      "|[319, 313]|  Rap| 0.5333333611488342|\n",
      "|[313, 287]|  Rap|0.20000000298023224|\n",
      "|[310, 312]|  Rap| 1.1666666269302368|\n",
      "|[308, 321]|  Rap|                0.5|\n",
      "|[293, 312]|  Pop|                1.5|\n",
      "|[305, 310]|  Rap| 0.3333333432674408|\n",
      "|[313, 298]|  Rap|                0.5|\n",
      "|[290, 311]|  Rap| 0.6666666865348816|\n",
      "|[315, 293]|  Rap|                4.0|\n",
      "|[294, 320]|  Rap|                1.0|\n",
      "|[307, 312]|  Rap| 1.2000000029802322|\n",
      "|[312, 310]|  Rap|                3.0|\n",
      "|[297, 319]|  Rap|  5.400000095367432|\n",
      "+----------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "447\n",
      "([312, 307], [318, 317], [300, 305], [300, 309], [296, 304])\n",
      "(0.6666666865348816, 0.8333333134651184, 0.3333333432674408, 0.75, 0.3333333432674408)\n",
      "Time taken is 10.522791862487793\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "cover.import_data(filename)\n",
    "cover.fit_transform(column_name=column_names, min_occurrence_count=5, window_size=5)\n",
    "cover.build_co_occurrence_matrix()\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, ArrayType\n",
    "from pyspark.sql.functions import col, explode, sum as sum_ \n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"age\", MapType(ArrayType(IntegerType()), IntegerType()), False),\n",
    "    StructField(\"type\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark_session.createDataFrame([({(1, 2): 1, (2 , 2): 2}, \"one\"), ({(1, 2): 1}, \"two\"), ({(2, 2): 3} , \"one\"), ({(2, 2): 3} , \"two\")], schema)\n",
    "\n",
    "df.show(10, False)\n",
    "\n",
    "t = df.select(explode(col(\"age\")), \"type\").groupBy(col(\"key\"), col(\"type\")).agg(sum_(\"value\").alias(\"value\"))\n",
    "\n",
    "t.show()\n",
    "\n",
    "x = t.select(\"value\", \"key\", \"type\").rdd.map(lambda x: (x.key, x.type, x.value)).groupBy(lambda x: x[1]).mapValues(list).collect()\n",
    "\n",
    "print(x)\n",
    "\n",
    "x = [zip(*y[1]) for y in x]\n",
    "\n",
    "\n",
    "index, genre, value = x[1]\n",
    "\n",
    "print(index)\n",
    "\n",
    "\n",
    "#MapType(ArrayType(IntegerType()), IntegerType())\n",
    "\n",
    "print(value)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
