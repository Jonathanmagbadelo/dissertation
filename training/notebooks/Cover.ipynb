{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n",
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "print(os.environ.get('PYSPARK_DRIVER_PYTHON'))\n",
    "print(os.environ.get('PYSPARK_PYTHON'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Cover\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col, explode, monotonically_increasing_id, when, split, lower\n",
    "\n",
    "class Cover:\n",
    "    def __init__(self, window_size=5, min_occurrence_count=1):\n",
    "        self.window_size = window_size\n",
    "        self.min_occurrence_count = min_occurrence_count\n",
    "        self.transformed_data = []\n",
    "        self.corpus = None\n",
    "    \n",
    "    def import_data(self, filename):\n",
    "        self.corpus = spark.read.format(\"csv\").option(\"header\", \"True\").option(\"mode\", \"DROPMALFORMED\").load(filename)\n",
    "        print(\"Corpus has {} documents\".format(self.corpus.count()))\n",
    "        \n",
    "    def fit_transform(self, column_name):\n",
    "        if self.corpus is None:\n",
    "            print()\n",
    "        else:\n",
    "            tokenise = udf(lambda x: x.lower().translate(str.maketrans('','',punctuation)).split(' ') if x else [], ArrayType(StringType()))\n",
    "            tokenised_dataframe = self.corpus.withColumn('tokens', tokenise(column_name).alias('tokens'))\n",
    "\n",
    "            words_dataframe = tokenised_dataframe.withColumn('word', explode(col('tokens')))\\\n",
    "                                 .groupBy('word')\\\n",
    "                                 .count()\\\n",
    "                                 .sort('count', ascending=True)\n",
    "            \n",
    "            #Need to find a way to automatically assign ID's from 1 - vocab size\n",
    "            words_with_id_dataframe = words_dataframe.withColumn('id', monotonically_increasing_id() + 1)\n",
    "\n",
    "            filtered_words_with_id_dataframe = words_with_id_dataframe.withColumn('id', when(words_with_id_dataframe['count'] <= self.min_occurrence_count, 0).otherwise(words_with_id_dataframe.id))\n",
    "            \n",
    "            token_to_id = filtered_words_with_id_dataframe.rdd.map(lambda r : (r.word,r.id)).collectAsMap()\n",
    "            \n",
    "            print(\"There are {} unique words\".format(len(token_to_id)))\n",
    "            \n",
    "            get_id = udf(lambda x: (token_to_id[word] for word in x), ArrayType(StringType()))\n",
    "            transformed_dataframe = tokenised_dataframe.withColumn('transform', get_id('tokens').alias('transform'))\n",
    "            \n",
    "            #transformed_dataframe.select('transform').show(50)\n",
    "                \n",
    "    def build_cooccur_matrix(self):\n",
    "        ij_list = []\n",
    "        cooccur_matrix = np.fromiter(())      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 5100 documents\n",
      "There are 42181 unique words\n",
      "Time taken is 6.296144962310791\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "filename = '/opt/training/data/raw/billboard_lyrics_1964-2015.csv'\n",
    "column_name = 'lyrics'\n",
    "cover = Cover(min_occurrence_count=5)\n",
    "\n",
    "start_time = time.time()\n",
    "cover.import_data(filename)\n",
    "cover.fit_transform(column_name)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Time taken is {}\".format(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "cover = Cover()\n",
    "texts = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "sentences = [\" \".join(list_of_words) for list_of_words in texts]\n",
    "\n",
    "start_time = time.time()\n",
    "data = cover.fit_transform(sentences)\n",
    "end_time = time.time()\n",
    "print(\"Time taken is {}\".format(end_time-start_time))\n",
    "print(data[1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
